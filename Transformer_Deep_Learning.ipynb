{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Transformers: https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=demystifying-bert-groundbreaking-nlp-framework\n",
    "Sample Code: https://www.tensorflow.org/text/tutorials/transformer\n",
    "Prerequisites:\n",
    "    Text summarization: https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/?utm_source=blog&utm_medium=understanding-transformers-nlp-state-of-the-art-models\n",
    "    Seq-2-Seq Attention: https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/?utm_source=blog&utm_medium=understanding-transformers-nlp-state-of-the-art-models\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.layers.experimental import sh\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using RNN: https://www.tensorflow.org/text/tutorials/text_generation\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x\n",
    "\n",
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  16896     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  67650     \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 12,  9, 57,  4,  4, 23, 11, 58, 53, 20,  8,  2, 47, 22,  1, 13,\n",
       "       43, 35, 11, 64, 16, 54, 30, 28,  7,  6, 46, 60,  9, 50,  6, 63, 52,\n",
       "       40, 11, 13, 16, 50,  6, 57, 10, 35,  9, 34, 55, 34, 27, 15, 30, 49,\n",
       "       61, 58, 57, 29, 30, 14,  8, 32, 22, 20,  5, 39, 29, 38, 28, 41, 40,\n",
       "       12, 63,  7, 27, 17, 55, 24, 62, 15, 36, 39,  2, 34, 61,  9,  4, 10,\n",
       "       20, 14, 21, 56, 49, 17, 23,  4, 51, 12, 49, 53, 20, 63, 30],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"Let's levy men, and beat him back again.\\n\\nCLARENCE:\\nA little fire is quickly trodden out;\\nWhich, bei\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"g;.r$$J:snG- hI\\n?dV:yCoQO,'gu.k'xma:?Ck'r3V.UpUNBQjvsrPQA-SIG&ZPYOba;x,NDpKwBWZ Uv.$3GAHqjDJ$l;jnGxQ\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.1896195\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.99767"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 471s 3s/step - loss: 2.7267\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 484s 3s/step - loss: 1.9859\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 505s 3s/step - loss: 1.7109\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 518s 3s/step - loss: 1.5510\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 516s 3s/step - loss: 1.4524\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 522s 3s/step - loss: 1.3849\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 522s 3s/step - loss: 1.3316\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 577s 3s/step - loss: 1.2875\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 3994s 3s/step - loss: 1.2462\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 440s 3s/step - loss: 1.2058\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 446s 3s/step - loss: 1.1668\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 512s 3s/step - loss: 1.1267\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 459s 3s/step - loss: 1.0841\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 493s 3s/step - loss: 1.0378\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 52169s 305s/step - loss: 0.9906\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 530s 3s/step - loss: 0.9408\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 507s 3s/step - loss: 0.8889\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 527s 3s/step - loss: 0.8365\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 513s 3s/step - loss: 0.7860\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 522s 3s/step - loss: 0.7367\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "What maid, he that is with you? here are fretting water to advance?\n",
      "\n",
      "QUEEN MAWARD:\n",
      "Thou bear'st the begg'd not with thy lanks there is so cannot know\n",
      "The heavens by mangled that Husband, and\n",
      "My love! thou shalt be recompense.\n",
      "\n",
      "SOMERSET:\n",
      "I told you age! I take her pacent and all; but, my good lord,\n",
      "But make your matter, son the oracle\n",
      "Garler seest thou with a goldnend; pauling adversary.\n",
      "\n",
      "GLOUCESTER:\n",
      "I go, and thou arrove subded the steem.\n",
      "\n",
      "ROMEO:\n",
      "Well, bear you gone, and then yet step and hearth through wants\n",
      "Fill those have went thee unloosed than his passage\n",
      "To look on me! Near, do.\n",
      "I pardon here.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Sweet York, enounce: the vice is for the dead.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "What said Clarence, do; away?\n",
      "\n",
      "First Autor ere you kill thy mother?\n",
      "\n",
      "CLIFFORD:\n",
      "For Warwick and lasses cry, Capules about;\n",
      "Do yet at Saint Alban's last on eye!\n",
      "\n",
      "BENVOLIO:\n",
      "A black, my lord; therefore darry. Now, seeing il\n",
      "Verious for a halp, he will unto you.\n",
      "\n",
      "ANGELO:\n",
      "\n",
      "Tid thou say'st me?\n",
      "\n",
      "BIONDELLO:\n",
      "Sir, yo \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 6.231527328491211\n"
     ]
    }
   ],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThere stands the heavy offer in this wilter queen\\nBy this resign of Warwick's bloody king.\\nAnd how she had for my despair! more than my\\npreyers considering this seven sick about me to't?\\n\\nNORTHUMBERLAND:\\nFearing, not reason me!\\n\\nSICINIUS:\\nCome on, benot thy life and din.\\n\\nCORIOLANUS:\\nA gentleman, a good one-day is too mile!\\nAfter more sole three pourth in a tabour\\nAnd, after more adone again.\\n\\nELBOW:\\nFie, fie, fie! 'tis now I know me doubted\\nWhose robes duty may we enough unto's name?\\n\\nVOLUMNIA:\\nDo you not, 'tis so: I am bound to have a hack-tremble;\\nHe hath made gradly winged for joys or bleed;\\nThat Montague is but posterity.\\n\\nGLOUCESTER:\\nThis is the matter,--nust, his burning kings;\\nAnd, as I told you mercy; I must thou hast\\nsaid, the army more impartial sin said too:\\nTheir men of suffer of himself through my\\ninfectalor,--\\nBeing both a bawd o'clock.\\n\\nPRINCE EDWARD:\\nAll mouther and thy poor is all; I do not seen them,\\nBeseech you, pawning, please my heavy son,\\nWhich is a poor fellow \"\n",
      " b\"ROMEO:\\nThen have you never sen down away before:\\nThe senators of Buckingham are the instrumen\\nTo ring it with such victory brows:\\nSuch extremess dealing, both in either, bless\\nTrust in the right of endel them.\\nWhen the dead I'll visit him to yourself?\\nAnd what our sunjus reise of Tybalt, whose holloa\\nMakes him for the both in some fire; but to come to-night.\\nHeaven sets me, and for these sad town, oug wit? our laps be bone?\\nThou slewest my head by day. For them I shall;\\nFie, fine upon your weapon, throw away'd.\\n\\nCLIFFORD:\\nWho father come?\\n\\nALONSO:\\nNot Romeo be, in the flesh and Claudio\\nBut to throw away outward souls! with nost for joy\\nShall ho mistake me with my helf!--\\nHe but dislanded in the prison,\\nWho carest our soldiers, and stand up,\\nThat when he babe's both cousin.\\n\\nRIVERS:\\nWhat sayest thou, Month of ghiet, how shall I do?\\n\\nPRINCE EDWARD:\\nWhat will you go?\\n\\nHERMIONE:\\nGo hom, to give; I'ld say not\\nlike the senate: it is my wayor for prayers' soldiers;\\nIf mere chased men between his ha\"\n",
      " b\"ROMEO:\\nWhy, therefore let us hence for rule the need.\\n\\nDUKE VINCENTIO:\\nNay, rather, will you stay my best of all\\ncontrovity?\\n\\nPage:\\nHere, hell first, Warwick and by counterfeit,\\nBetter it is but every officer.\\n\\nHENRY BOLINGBROKE:\\nFirst, know, my lord, this is your sister.\\n\\nDUKE VINCENTIO:\\nThis fresh as virtuous Lady Bona, Alcharse him with her love!'\\n'Signifrains done envy; he\\nthat was minute of it: their new-ready sculd\\nOf Willingness; not a Richard throne!\\nTo high, to lock up so shriff\\nAnd springle heart.\\n\\nEDWARD:\\nYou must return upon thing prattling feat.\\n\\nDUKE OF YORK:\\nWhat such a counterfeit traitor have more welcome?\\n\\nCLARENCE:\\nHe does make them received; therefore accept a motion morning;\\nBut whet's our ancient counins, his son wrongs and\\nreportuned here to make cold. Kate threw this summoth after?\\nyoughtable, what to cats have renistinct their yet\\nopen'd it.\\n\\nDUKE VINCENTIO:\\nMarry, and, life doth nothing warm those that thou look'st,\\nAnd like your side, to hide her bounds;\\nAnd shewar\"\n",
      " b\"ROMEO:\\nI would wish you all.\\n\\nVessenger:\\nSir, am Carelet jollow; but insolent note.\\nHearery, the matter? pieculer doth the prince my sweet sorrow.\\n\\nMARIANA:\\nProvost, a good meaning.\\n\\nProvost:\\nNow, fellow, go; be poor into finds:\\nMy fill contenved of Bianca's starws.\\n\\nPAMERLA:\\nDo not see this very heart of what there's mistress!\\nMake me with joy?\\n\\nTurst:\\nAy, my good brother Marcius.\\n\\nFirst Servingman:\\nNay; rather king! o' Gentle Warwick,\\nThat seek revenge unto the one, or be decreased;\\nAnd though braw they reind on't, which wash'd\\nTo counterfect deceary hides:\\nThe gods grant shall not reason have the while.\\nYou'll not believe thy souly hars-heart's dear!\\nThough force again to her our joy,\\nWithout bootless to become a breath;\\nFor me no mother, nay, thou cuttivess slept.\\n3 KING HENRY VI\\n\\nGLOUCESTER:\\nFoully from the choler.\\n\\nProvost:\\n\\nISABELLA:\\nWhy, this must see the enema?\\n\\nNIRNES:\\nBe you come; thou art a wife!\\nDo I not so; an't like your prader,\\nAnd over him to revenge, turns are stood for;\\nTo\"\n",
      " b\"ROMEO:\\nNay, grant the saying now to give my tongue\\nAnd now remain makes us all, that thou dost say\\n'This he did follow to the earth to gut the hearing;\\nAbout your suitors to London,\\nTo stead the time and by judge and brought steal,\\nAs mice by day expect his country.\\n\\nWARWICK:\\nI crammanted thee, false hot well well.\\nHow I have talked my daughter good,\\nThat done the red blood of the king, to drine\\nAnd, as most sorth is past my six'\\nBecome the air did unto the serving-eat,\\nThen and else have I beg to hear thee was.\\n\\nEDWARD:\\nWhere is thy name, sweet weeds, and very good tongues,\\nFor though the bean money out of stronger\\nAs patient clouds; but for surely did:\\nFor Warwick attender her projuses him.\\n\\nHASTINGS:\\nO What, I say! give conscience all this way to--this man, boardeal the\\nrevenge for every horse. Take edds\\n\\nButtise: I may something macker.\\n\\nANGELO:\\nHold, God he be and ricenanty is an\\nirnob, night, he'er I.\\nAnd how shall waich the reckless of the rebels\\nThat slew thy slaughter'd Rome is guil\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 5.299328565597534\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001F45E069908>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Sometime she droops, or was I'ld through every\n",
      "pretty days.\n",
      "'Praves; be ruled; and, now I hear you \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 365s 2s/step - loss: 2.7040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f4006c8710>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}\n",
    "\n",
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1415\n",
      "Epoch 1 Batch 50 Loss 2.0766\n",
      "Epoch 1 Batch 100 Loss 1.9253\n",
      "Epoch 1 Batch 150 Loss 1.8581\n",
      "\n",
      "Epoch 1 Loss: 1.9812\n",
      "Time taken for 1 epoch 412.00 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.8042\n",
      "Epoch 2 Batch 50 Loss 1.7188\n",
      "Epoch 2 Batch 100 Loss 1.6794\n",
      "Epoch 2 Batch 150 Loss 1.6599\n",
      "\n",
      "Epoch 2 Loss: 1.7084\n",
      "Time taken for 1 epoch 493.25 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.5919\n",
      "Epoch 3 Batch 50 Loss 1.5972\n",
      "Epoch 3 Batch 100 Loss 1.5532\n",
      "Epoch 3 Batch 150 Loss 1.5450\n",
      "\n",
      "Epoch 3 Loss: 1.5483\n",
      "Time taken for 1 epoch 500.85 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.4731\n",
      "Epoch 4 Batch 50 Loss 1.4601\n",
      "Epoch 4 Batch 100 Loss 1.4364\n",
      "Epoch 4 Batch 150 Loss 1.4006\n",
      "\n",
      "Epoch 4 Loss: 1.4499\n",
      "Time taken for 1 epoch 491.38 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3691\n",
      "Epoch 5 Batch 50 Loss 1.3639\n",
      "Epoch 5 Batch 100 Loss 1.3733\n",
      "Epoch 5 Batch 150 Loss 1.3379\n",
      "\n",
      "Epoch 5 Loss: 1.3815\n",
      "Time taken for 1 epoch 476.25 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3334\n",
      "Epoch 6 Batch 50 Loss 1.3262\n",
      "Epoch 6 Batch 100 Loss 1.2980\n",
      "Epoch 6 Batch 150 Loss 1.3668\n",
      "\n",
      "Epoch 6 Loss: 1.3299\n",
      "Time taken for 1 epoch 466.05 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2571\n",
      "Epoch 7 Batch 50 Loss 1.2928\n",
      "Epoch 7 Batch 100 Loss 1.2826\n",
      "Epoch 7 Batch 150 Loss 1.2749\n",
      "\n",
      "Epoch 7 Loss: 1.2852\n",
      "Time taken for 1 epoch 428.35 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.1946\n",
      "Epoch 8 Batch 50 Loss 1.2340\n",
      "Epoch 8 Batch 100 Loss 1.2430\n",
      "Epoch 8 Batch 150 Loss 1.2922\n",
      "\n",
      "Epoch 8 Loss: 1.2439\n",
      "Time taken for 1 epoch 439.72 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.2037\n",
      "Epoch 9 Batch 50 Loss 1.1777\n",
      "Epoch 9 Batch 100 Loss 1.1943\n",
      "Epoch 9 Batch 150 Loss 1.2058\n",
      "\n",
      "Epoch 9 Loss: 1.2047\n",
      "Time taken for 1 epoch 389.27 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1682\n",
      "Epoch 10 Batch 50 Loss 1.1700\n",
      "Epoch 10 Batch 100 Loss 1.1704\n",
      "Epoch 10 Batch 150 Loss 1.1940\n",
      "\n",
      "Epoch 10 Loss: 1.1646\n",
      "Time taken for 1 epoch 412.05 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural machine translation with attention\n",
    "# https://www.tensorflow.org/text/tutorials/nmt_with_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Shape checker\n",
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    if isinstance(names, str):\n",
    "      names = (names,)\n",
    "\n",
    "    shape = tf.shape(tensor)\n",
    "    rank = tf.rank(tensor)\n",
    "\n",
    "    if rank != len(names):\n",
    "      raise ValueError(f'Rank mismatch:\\n'\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\n",
    "                       f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "      if isinstance(name, int):\n",
    "        old_dim = name\n",
    "      else:\n",
    "        old_dim = self.shapes.get(name, None)\n",
    "      new_dim = shape[i]\n",
    "\n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path_to_file.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "  inp = [inp for targ, inp in pairs]\n",
    "  targ = [targ for targ, inp in pairs]\n",
    "\n",
    "  return targ, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.\n"
     ]
    }
   ],
   "source": [
    "targ, inp = load_data(path_to_file)\n",
    "print(inp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.\n"
     ]
    }
   ],
   "source": [
    "print(targ[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Tenemos un cocinero excelente.'\n",
      " b'Sencillamente quiero o\\xc3\\xadr tus razones.'\n",
      " b'\\xc3\\x89l y t\\xc3\\xba sois ambos muy amables.'\n",
      " b'\\xc2\\xbfQu\\xc3\\xa9 le ha puesto tan triste?'\n",
      " b'\\xc2\\xbfQu\\xc3\\xa9 dijiste ayer?'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'We have a very good chef.' b'I just want to hear your reasons.'\n",
      " b'You and he are both very kind.' b'What made her so sad?'\n",
      " b'What did you say yesterday?'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "  print(example_input_batch[:5])\n",
    "  print()\n",
    "  print(example_target_batch[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?'\n",
      "b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accecented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Todavía está en casa?\n",
      "[START] ¿ todavia esta en casa ? [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'que', 'de', 'el', 'a', 'no']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "\n",
    "# Here are the first 10 words from the vocabulary:\n",
    "input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to', 'you', 'tom']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor = preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(targ)\n",
    "output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[   2,  157,   16, 3322, 1581,    4,    3,    0,    0,    0],\n",
       "       [   2, 1802,   48,  509,  183, 2257,    4,    3,    0,    0],\n",
       "       [   2,    7,   33,   36, 1386,  823,   42,    1,    4,    3]],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] tenemos un cocinero excelente . [END]           '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(example_tokens)\n",
    "plt.title('Token IDs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.enc_units = enc_units\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "\n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # The GRU RNN layer processes those vectors sequentially.\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   # Return the sequence and state\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, tokens, state=None):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding for each token.\n",
    "    vectors = self.embedding(tokens)\n",
    "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "    # 3. The GRU processes the embedding sequence.\n",
    "    #    output shape: (batch, s, enc_units)\n",
    "    #    state shape: (batch, enc_units)\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "    shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "    # 4. Returns the new sequence and its state.\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch, shape (batch): (64,)\n",
      "Input batch tokens, shape (batch, s): (64, 18)\n",
      "Encoder output, shape (batch, s, units): (64, 18, 1024)\n",
      "Encoder state, shape (batch, units): (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Convert the input text to tokens.\n",
    "example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    # For Eqn. (4), the  Bahdanau attention\n",
    "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "  def call(self, query, value, mask):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(query, ('batch', 't', 'query_units'))\n",
    "    shape_checker(value, ('batch', 's', 'value_units'))\n",
    "    shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "    # From Eqn. (4), `W1@ht`.\n",
    "    w1_query = self.W1(query)\n",
    "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "    # From Eqn. (4), `W2@hs`.\n",
    "    w2_key = self.W2(value)\n",
    "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "    value_mask = mask\n",
    "\n",
    "    context_vector, attention_weights = self.attention(\n",
    "        inputs = [w1_query, value, w2_key],\n",
    "        mask=[query_mask, value_mask],\n",
    "        return_attention_scores = True,\n",
    "    )\n",
    "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 18])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(units)\n",
    "(example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch_size, query_seq_length, units):           (64, 2, 1024)\n",
      "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 18)\n"
     ]
    }
   ],
   "source": [
    "# Later, the decoder will generate this attention query\n",
    "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# Attend to the encoded tokens\n",
    "\n",
    "context_vector, attention_weights = attention_layer(\n",
    "    query=example_attention_query,\n",
    "    value=example_enc_output,\n",
    "    mask=(example_tokens != 0))\n",
    "\n",
    "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Mask')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAb3UlEQVR4nO3df5RdZX3v8fcnk4T8ICEkkJAEYto0i4I/iJgi4G1F0YriMvG2YWlp74Bpg6tqbbELY5e9qNe2tFar9brKihZJtSgpFYldXSBNC6ggSnop/gAFNISQHxNCYiBCfs33/rF3msMwc/bOnH32Oc+Zz2utWXPO3vvs/ZzJN995zneeZz+KCMzMLD3jOt0AMzMbHSdwM7NEOYGbmSXKCdzMLFFO4GZmiXICNzNLlBN4m0m6VtKfdrodw5H0q5J+VPLYCyRtaXebzAAk3SHpdzvdjm7Xkwk8/8ffLem4Ids3SXpdw/OFkkLS+Ique5mkbzZui4h3RsT/qeL8VYuIb0TE6VWcS9L1kj5axbksDfn/pwOSThqy/f78/9XCzrRs7Oi5BJ4Hza8CAbylo40x630/Bd5+5ImklwKTO9ecsaXnEjjwv4BvA9cD/Uc2SvoCsAD4mqRnJF0F3JXv3pNvOy8/9h2SHsx78bdJelHDeULSOyU9nO//jDJnANcC5+Xn2pMf/7yeqaTfk/SIpKckrZc0r+jcQ9+gpEmSnj3S85H0QUmHJE3Pn39U0ifzx8dJ+mtJmyXtyEs6k/N9zyuLSDpb0v+T9LSkf5J049BetaT3SRqQtE3S5fm2VcClwFX5e/9avv39kp7Iz/cjSRceyz+kJeELZP/njugH/uHIE0kX5zG1V9Ljkj7UsG+SpC9K2iVpj6TvSpoz9AKS5kp6QNIft/ONJCkieuoLeAT4feAVwEFgTsO+TcDrGp4vJOupj2/Ytjw/xxnAeOCDwN0N+wP4F2AG2S+EncBF+b7LgG8Oac/1wEfzx68FngTOBo4DPg3cVebcw7zPu4DfyB9/HXgUeGPDvrfmjz8JrAdmAtOArwF/ke+7ANiSP54IPAa8F5gA/E/gQEPbLwAOAR/J978J+Dlw4tD3mT8/HXgcmNfws17U6fjwV6X/1zYBrwN+lP9/6cv/zV+Ux/LCPG5eStZZfBmwA1iev/6KPB6n5K99BTA933cH8Lv5OX4MrOr0++3Gr57qgUv6H2TBsy4iNpIltd86xtNcQZbgHoyIQ8CfA0sae+HANRGxJyI2A/8BLCl57kuB6yLiPyNiP/ABsh77wlGc+07g1Xn9/mXA3+bPJwG/Anwj773/HvBHEfFURDydv5+3DXO+c8l+Yf1tRByMiK8A3xlyzEHgI/n+fwWeIUvUwzlM9kvqTEkTImJTRDw60g/GknakF/564CHgiSM7IuKOiPheRAxGxAPAl4BX57sPArOAX4qIwxGxMSL2Npz3TLJEfnVErKnhfSSnpxI42ce3r0fEk/nzG2goo5T0IuBT+Ue6PcBTgID5Dcdsb3j8c+D4kueeR9bLBSAingF2jfLcd5L1bs4GvgfcTvYf41zgkfxncDJZ72Zjw/u5Nd8+XNueiLz7k3t8yDG78l9qhe2LiEeAPwQ+BAxI+nJjuch6yhfIOkqX0VA+AZD0Skn/IWmnpJ8B7wROanjdbcCXJW2V9FeSJjS8/FKyXwY3tfsNpKpnEnhe172ErBe6XdJ24I+AsySdlR829NaLw92K8XHgioiY0fA1OSLuLtGMols7biX7BXGkzVPJeiBPjPiKkd1N1vt9K3BnRPyQrOxyMVlyh6xc8yzw4ob3ckJEDJd0twHzh9TcTzuG9rzgvUfEDRFx5FNRAH95DOezRETEY2R/zHwT8JUhu28gK+GdFhEnkP2dSPnrDkbEhyPiTOB84M08v57+IbIYvkFSX1vfRKJ6JoGT1a4Pk33sWpJ/nQF8g6NBsQP4xYbX7AQGh2y7FviApBcDSDpB0oqSbdgBnCpp4gj7bwAul7RE2RDHPwfujYhNJc//3yLi58BG4F0cTdh3k5WA7syPGQQ+C/yNpNn5+5kv6Q3DnPIesp/fuyWNl7QMOOcYmvS8n62k0yW9Nn+fz5H9Ijl8DOeztKwEXhsR+4ZsnwY8FRHPSTqHhpKmpNdIemmenPeSlVQaY+QgsAKYCnxBUi/lq0r00g+kH/h8RGyOiO1HvoD/C1ya14r/AvhgXk744zwJ/hnwrXzbuRFxM1lP8cuS9gLfB95Ysg3/DvwA2C7pyaE7I2ID8KfAP5P1eBcxfD26rDvJ/qD4nYbn0zg6ugbg/WR/lP12/n7+jWHq1hFxgOwPlyuBPcBvk/1BdX/Jtvw9Wb17j6SvktW/ryHrQW0HZgN/cixvztIREY9GxH3D7Pp94COSngb+N7CuYd8pZOWRvcCDZPH7xSHnPRKXs4HrnMSfT88veZodJele4NqI+Hyn22JmL+TfZvbfJL1a0il5CaWfbHTLrZ1ul5kNr5Ip5NYzTif7iHs82RDM34yIbZ1tkpmNxCUUM7NEuYRiZpaoWkso4ydNjYnTZtZ5yRd6wZ1FRqGHPrSMf3LoqK90Pc3uJyNiuElKbXfSzL5YeNqE4gOtNj9+YEqnm1CZkWK71gQ+cdpMfvk3rhz5gKLEWEXyrSuB1/FeKjBrTZn5SWn4t7jpseKj2mPhaRP4zm0LOnV5G8Yb5p1VfFAiRoptl1DMzBLlBG5mlqhaSyjRB/tPaHJAUVmhirLDYIljin6tlSmhFLW1gjLMvI/1TvnDrFEvlT/ayT1wM7NEOYGbmSXKCdzMLFH1TqUXrdWxqxh/Xeb6VQwB7KGx4mbWndwDNzNLlBO4mVmiSpVQJM0APge8hKw48A6ylahvJFs1ehNwSUTsbnaewUnBvjMOjL61XTJ7sUx5ZPHlw93b3rpNVbE9Vnh4X3cp2wP/FHBrRPwycBbZ6hmrgQ0RsRjYkD83S41j25JVmMAlTQd+jWzJLCLiQETsAZYBa/PD1pKtSWmWDMe2pa5MCeUXyRb//Xy+uvtG4L3AnCM3+4+IbUcWzR1K0ipgFcCE6Scy9cGR1vvtIhXMotx61fmtt8MzMdutstheMH9srI1y29b/quU6LtWUU6aEMh44G/i7iHg5sI9j+EgZEWsiYmlELO2bMnWUzTRri8pi++RZfe1qo9mIyiTwLcCWiLg3f34TWdDvkDQXIP8+0J4mmrWNY9uSVpjAI2I78Lik0/NNFwI/BNYD/fm2fuCWtrTQrE0c25a6soW79wD/KGki8BPgcrLkv07SSmAzsKLoJIPj4dlTOjxFsa6hiAVvc9GV99TTDitSSWzbUa5f16dUAo+I+4Glw+y6sNrmmNXLsW0p80xMM7NE1Tr2adxBmLytSQ2jjgUd6lqMoUCpYYYeRmgJqmKoocsw5bgHbmaWKCdwM7NEOYGbmSWq1hq4Asbvb+EEFdSvo0QdXUXXqWthiQID72leR5/9adfILU1FdXTXyDPugZuZJcoJ3MwsUfXfQq3TEzG7pPxhZtYq98DNzBLlBG5mlqhaSyiHpwQ/e0WzNTErqG8UDTOpaSbm4nd4TUzrPR790V3cAzczS5QTuJlZopzAzcwSVe8wwsNi3O4Jo399mfp10TFlhgBWMEzw0U+c13S/F3SwFJW506Dr5PVxD9zMLFFO4GZmiaq3hNIXDM44OPrXlyptFNRQSt3NqrVLACy+3MMIrfe4PNJd3AM3M0uUE7iZWaKcwM3MElXvosbPiakPThz9CcrUwOu422CJa5RatLiIFzW2LlPFgsVluNZejnvgZmaJcgI3M0tUqRKKpE3A08Bh4FBELJU0E7gRWAhsAi6JiN3taWauw4tBWO/pmtg2G4Vj6YG/JiKWRMTS/PlqYENELAY25M/NUuTYtiS1UkJZBqzNH68FlrfeHLOu4Ni2JJRN4AF8XdJGSavybXMiYhtA/n32cC+UtErSfZLuO/zzfc2vooIvs+pVEts7dx2uqblmR5UdRviqiNgqaTZwu6SHyl4gItYAawAmzz3NVWzrNpXE9tKzJjm2rXaleuARsTX/PgDcDJwD7JA0FyD/PtCuRpq1i2PbUlaYwCVNlTTtyGPg14HvA+uB/vywfuCWdjXSrB0c25a6MiWUOcDNko4cf0NE3Crpu8A6SSuBzcCKohMNTgr2ndFkUePCOneJT6ll7jZYxHcjHCsqi+2xwjMku0thAo+InwAv+FeLiF3Ahe1olFkdHNuWOs/ENDNLVK03s5qwV5xye5M1MaOGP+SruMRSVIVRiWbuvbT5mphVKNOOlpX4N5n2pW/X0BDrBnXdzKobpFAucg/czCxRTuBmZolyAjczS1StNfCD04Ptr2+yqHEFw/dqmXLvYYQ2RqVQFx5L3AM3M0uUE7iZWaJqLaHogDhucwtrYiZk84dbXxNzwdVe89K6SxXDCF2GqY574GZmiXICNzNLVK0llHEHYOoTTQ4oGt1RZoRJHSNZqpgBWeK97FrVehmmyKw1LtNYveqazTkWSjXugZuZJcoJ3MwsUU7gZmaJqrcGfgimDLR38dc67tBXxZoRpdRwnWff+srmB9S00uPkr95bz4VszOiGOye2uw7vHriZWaKcwM3MElXvzaxmBFuWNymhVDI8r4qxiAV8Mysbo8bC0LyUuAduZpYoJ3Azs0Q5gZuZJarWGjjQWp27ijGC3TKV3sysRe6Bm5klygnczCxRpUsokvqA+4AnIuLNkmYCNwILgU3AJRGxu+lJDolxuyc0uUhBI2q6C2BhqabEVMxHP35e0/2L3ndPiYZYu1US12NImdmNHmpYn2Ppgb8XeLDh+WpgQ0QsBjbkz81S47i2ZJVK4JJOBS4GPteweRmwNn+8FlhebdPM2stxbakrW0L5JHAVMK1h25yI2AYQEdskzR7uhZJWAasAJkw/kcnb67oT1AhKXD7U/CANtt6MrVeVWKyhoJIz72NejKFFo45reH5sL5hf/4CubuV1M+tT2AOX9GZgICI2juYCEbEmIpZGxNK+KVNHcwqzyrUa1/D82D55Vl+FrTMrp0y34VXAWyS9CZgETJf0RWCHpLl5L2UuMNDOhppVzHFtySvsgUfEByLi1IhYCLwN+PeI+G1gPdCfH9YP3NK2VppVzHFtvaCVwt01wDpJK4HNwIqiFwxOCvadcWDkAwrr01VMo6xAiRr44nf4boSJOua4Hktcm+4ux5TAI+IO4I788S7gwuqbZFYvx7WlyjMxzcwSVe/Yp26YiVlGBVWYRz9RMBPzSs/EtPR4JmZ3cQ/czCxRTuBmZolyAjczS1StNfC+52DGQ90/lb5Qman0BdfZdUXrU+mreP2sz3o6vlWriqn0VRgLtXj3wM3MEuUEbmaWqHpLKAeC6Y8dHHF/iXUSekeZ91pQAqlg3QkOXPQrJRrSeaWWQ731pra3w9LRLaWcKvTNHX67e+BmZolyAjczS1StJZSDx4tt5zeZiVmDMmWFUh/Xa7Dgao8Qsd4zFkaHVO/hYbe6B25mligncDOzRDmBm5klasytxFqmvl1UJ++WGrmZjW3ugZuZJcoJ3MwsUfXOxNwP03/a5vpD0ekrmAFZiRLt2HNZ80UhKlHwXmes9cITVq1umSHZC8MZ3QM3M0uUE7iZWaKcwM3MElVrDfzQZNj1siYHdMvdCKuooxcs+rDofa4tW2/qhdpyKtwDNzNLlBO4mVmiChO4pEmSviPpvyT9QNKH8+0zJd0u6eH8+4mFV1PBVxWigq8q1PFerSWVxrZZB5Tpge8HXhsRZwFLgIsknQusBjZExGJgQ/7cLCWObUtaYQKPzDP50wn5VwDLgLX59rXA8ra00KxNHNuWulKjUCT1ARuBXwI+ExH3SpoTEdsAImKbpNkjvHYVsAqgb9YMBmeMvCZmYWmhTHmjioUiK2jH4svvKz7IOq6q2F4wf2zcF84jTLpLqT9iRsThiFgCnAqcI+klZS8QEWsiYmlELO2bNnW07TRri6pi++RZfe1rpNkIjmkUSkTsAe4ALgJ2SJoLkH8fqLx1ZjVxbFuKyoxCOVnSjPzxZOB1wEPAeqA/P6wfuKVdjTRrB8e2pa5M4W4usDavFY4D1kXEv0i6B1gnaSWwGVhReKZDYtzuJosaVzEDsoo6+rjWxxI++onmdxJcdKVnYnaB6mJ7jChzJ0HXyetTmMAj4gHg5cNs3wVc2I5GmdXBsW2p80xMM7NE1bugwwGY9tMmNY4qyh9FSpRhQq1PlSwazTjwB+e3fI2in8fsT9/d+jXMjlEdCza4TJNxD9zMLFFO4GZmiXICNzNLVK018MNTgp+94sDIB9RU4y7W+nR8T6W3XuTac3dxD9zMLFFO4GZmiar3FmqHxLinmszErORuhBWco9VrAI9+vGAmptfEtAR5JmZ3cQ/czCxRTuBmZomqtYQy7hBM3t7CTMwyqlrTss22XlViJmbBe5n3Mc+0tO5TxUxMl2HKcQ/czCxRTuBmZolyAjczS1StNfDB8fDsKU0Ku0U18MESF6niV1IVCyMXnMILOlivcv26Pu6Bm5klygnczCxR9c7EFK0NFSzz2qIyS5lfWWVKJGZmHeYeuJlZopzAzcwS5QRuZpaoWmvgE56BU76VyFz3Ziooke9bcW7xQT3wowKYetO3O90Eq1Edixp3i04PmXQP3MwsUU7gZmaJKiyhSDoN+AfgFLJBemsi4lOSZgI3AguBTcAlEbG72bkGJ8C+eQn8zigqXZSZiFlwTNFkz1IKzjH7075bYTNVxrbVq9Oli25RJpseAt4XEWcA5wLvknQmsBrYEBGLgQ35c7OUOLYtaYUJPCK2RcR/5o+fBh4E5gPLgLX5YWuB5e1qpFk7OLYtdcc0CkXSQuDlwL3AnIjYBtl/BEmzR3jNKmAVwMTZ05n4hp0jnn+woDYxrsSwDFUwQuRwQf1j5sU/bv0i1lVaje0F8+ud1NwuLk2kpXRBWtLxwD8DfxgRe8u+LiLWRMTSiFg6fvqU0bTRrK2qiO2TZ/W1r4FmIyiVwCVNIAvwf4yIr+Sbd0iam++fCwy0p4lm7ePYtpQVJnBJAv4eeDAiPtGwaz3Qnz/uB26pvnlm7ePYttSVKdy9Cvgd4HuS7s+3/QlwDbBO0kpgM7Ci6ESHDvWxc+cJo21rNcoM3yuoo+/6/NLCUyy+/L5y7bFOqiy2e0XRLErXyLtLYQKPiG8yckq7sNrmmNXHsW2pS2BWjZmZDafWsU8vnr6Tb73+2hH3H6cJLV9jfxxsuv8t84vLH2Ypcnlj7HEP3MwsUU7gZmaJcgI3M0tUrTXwh7bP5txr/qDOS75QictXsaZx0d0Gy1yj8I6FvhuhNfBCCmOPe+BmZolyAjczS1StJRQdhol7m3zuLyorlJhFWVi6qOBXVhXljzpKKHsuO6/4IgXnmLH2nuJzmNWsinJRL5Rh3AM3M0uUE7iZWaJqLaEcmhbsfE2zmZJVLEZ5LC0a5WVKXMM3s7Je1Atlh17iHriZWaKcwM3MEuUEbmaWqFpr4OOeE1N/OHHkA+qoX9d0jq3vP7/1kww23z3vY55pafWqa7ana+3luAduZpYoJ3Azs0TVW0I5/hBTXr1zxP2DBbWLcRXUWFSiPHK4YJrkzIt/3HI7zLqRSxdpcQ/czCxRTuBmZolyAjczS1StNfDYM57DXz151K8/3CVDBHddcVLxQd0wpb9EG2Z91kMR7aheWhRiLNTz3QM3M0uUE7iZWaIKSyiSrgPeDAxExEvybTOBG4GFwCbgkojYXXi1cXB4UrOLFTe4V3hNzM6rNLatVmOhPFJGmR749cBFQ7atBjZExGJgQ/7cLDXX49i2hBUm8Ii4C3hqyOZlwNr88VpgecXtMms7x7albrSjUOZExDaAiNgmafZIB0paBawC6DvxRJ7+hSaf+4tKBmU+L1SxGGUF60oUnWPRlV5rskuNKrYXzK91QFdXc3mjPm3/I2ZErImIpRGxtG/q1HZfzqw2jbF98qy+TjfHxqDRJvAdkuYC5N8HqmuSWUc5ti0Zo03g64H+/HE/cEs1zTHrOMe2JaPMMMIvARcAJ0naAlwNXAOsk7QS2AysKHOxcYdg8vYmBeQqhhGWqXG3fI0SxxQ0Y+tVrS/4MO+vPEywFVXGth1VxWxO19HLKUzgEfH2EXZdWHFbzGrl2LbUeSammVmiah37pEEY/1ydV+xtA+9pXobxTExLVVEZxiWWjHvgZmaJcgI3M0uUE7iZWaLqnf87CH2t1MArmMJem1YXY6jArlXFQxVnrXGd3NJTZqjiWKiTuwduZpYoJ3Azs0TVuybmeNg/o84rdlAls0qb7/ZMTOtVY6H8UQX3wM3MEuUEbmaWqFpLKIOTgn1nHmhyRAUrKVQxuqOCESSLL7+vgoaYdReXNrqLe+BmZolyAjczS5QTuJlZomqtgY97Tkz94cSRD+iC2YtVKVywoYJZpR5GaHWrYrGGMlxrL8c9cDOzRDmBm5klqt6bWQWdL4NUcUOsMu+h6FdjmXN0+mdlZl3NPXAzs0Q5gZuZJcoJ3MwsUfVOpZ8Az87tcGF3XInrRwW3Eiy4zKIr72n9GmZdyEMA6+MeuJlZopzAzcwS1VIJRdJFwKeAPuBzEXFN86sFgzMPjry/jupKqepIQUNKlFh8N8K0HXNsjxEuj3SXUffAJfUBnwHeCJwJvF3SmVU1zKxTHNuWilZKKOcAj0TETyLiAPBlYFk1zTLrKMe2JaGVEsp84PGG51uAVw49SNIqYFX+dP9j/au/38I163IS8GQrJ3isooYUaLmdNaqjrS+q6Dyjiu2+uQ+Pgdh+uLKGFEgltutq57Cx3UoCH64Q/ILicUSsAdYASLovIpa2cM1auJ3VS6mtOLY7zu0sp5USyhbgtIbnpwJbW2uOWVdwbFsSWkng3wUWS/oFSROBtwHrq2mWWUc5ti0Joy6hRMQhSe8GbiMbanVdRPyg4GVrRnu9mrmd1UumrY7truB2lqAI37PUzCxFnolpZpYoJ3Azs0TVksAlXSTpR5IekbS6jmuOlqRNkr4n6X5JXTMfXtJ1kgYkfb9h20xJt0t6OP9+YifbeMQIbf2QpCfyn+v9kt7UyTZWJZXY7ta4hnRiuxvjuu0JPNFpya+JiCVdNg71euCiIdtWAxsiYjGwIX/eDa7nhW0F+Jv857okIv615jZVLsHY7sa4hnRi+3q6LK7r6IF7WnIFIuIu4Kkhm5cBa/PHa4HltTZqBCO0tRc5tiuQSmx3Y1zXkcCHm5Y8v4brjlYAX5e0MZ8q3c3mRMQ2gPz77A63p8i7JT2QfxTt+EfiCqQU2ynFNaQV2x2L6zoSeKlpyV3kVRFxNtnH4ndJ+rVON6hH/B2wCFgCbAM+3tnmVCKl2HZct0dH47qOBJ7UtOSI2Jp/HwBuJvuY3K12SJoLkH8f6HB7RhQROyLicEQMAp+lu3+uZSUT24nFNSQS252O6zoSeDLTkiVNlTTtyGPg14FuvsPceqA/f9wP3NLBtjR15D9j7q1098+1rCRiO8G4hkRiu9Nx3fZFjUc5LblT5gA3S4LsZ3NDRNza2SZlJH0JuAA4SdIW4GrgGmCdpJXAZmBF51p41AhtvUDSErISwybgio41sCIJxXbXxjWkE9vdGNeeSm9mlijPxDQzS5QTuJlZopzAzcwS5QRuZpYoJ3Azs0Q5gZuZJcoJ3MwsUf8fNoz7YevgjO0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.pcolormesh(attention_weights[:, 0, :])\n",
    "plt.title('Attention weights')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pcolormesh(example_tokens != 0)\n",
    "plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 2, 18])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_slice = attention_weights[0, 0].numpy()\n",
    "attention_slice = attention_slice[attention_slice != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cd0b1f4c88>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.suptitle('Attention weights for one sequence')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "a1 = plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "# freeze the xlim\n",
    "plt.xlim(plt.xlim())\n",
    "plt.xlabel('Attention weights')\n",
    "\n",
    "a2 = plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(attention_slice)), attention_slice)\n",
    "plt.xlabel('Attention weights, zoomed')\n",
    "\n",
    "# zoom in\n",
    "top = max(a1.get_ylim())\n",
    "zoom = 0.85*top\n",
    "a2.set_ylim([0.90*top, top])\n",
    "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderInput(typing.NamedTuple):\n",
    "  new_tokens: Any\n",
    "  enc_output: Any\n",
    "  mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "  logits: Any\n",
    "  attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self,\n",
    "         inputs: DecoderInput,\n",
    "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "  shape_checker = ShapeChecker()\n",
    "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "  shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "  if state is not None:\n",
    "    shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "  # Step 1. Lookup the embeddings\n",
    "  vectors = self.embedding(inputs.new_tokens)\n",
    "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "  # Step 2. Process one step with the RNN\n",
    "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "  shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "  # Step 3. Use the RNN output as the query for the attention over the\n",
    "  # encoder output.\n",
    "  context_vector, attention_weights = self.attention(\n",
    "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "  attention_vector = self.Wc(context_and_rnn_output)\n",
    "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "  # Step 5. Generate logit predictions:\n",
    "  logits = self.fc(attention_vector)\n",
    "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "  return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target sequence, and collect the \"[START]\" tokens\n",
    "example_output_tokens = output_text_processor(example_target_batch)\n",
    "\n",
    "start_index = output_text_processor._index_lookup_layer('[START]').numpy()\n",
    "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
      "state shape: (batch_size, dec_units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Run the decoder\n",
    "dec_result, dec_state = decoder(\n",
    "    inputs = DecoderInput(new_tokens=first_token,\n",
    "                          enc_output=example_enc_output,\n",
    "                          mask=(example_tokens != 0)),\n",
    "    state = example_enc_state\n",
    ")\n",
    "\n",
    "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['places'],\n",
       "       ['cases'],\n",
       "       ['outdoors'],\n",
       "       ['convenient'],\n",
       "       ['oath']], dtype='<U16')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(output_text_processor.get_vocabulary())\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_result, dec_state = decoder(\n",
    "    DecoderInput(sampled_token,\n",
    "                 example_enc_output,\n",
    "                 mask=(example_tokens != 0)),\n",
    "    state=dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['program'],\n",
       "       ['cameras'],\n",
       "       ['natural'],\n",
       "       ['suit'],\n",
       "       ['songs']], dtype='<U16')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
    "first_word = vocab[sampled_token.numpy()]\n",
    "first_word[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss'\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(y_true, ('batch', 't'))\n",
    "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss = self.loss(y_true, y_pred)\n",
    "    shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    shape_checker(mask, ('batch', 't'))\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "    super().__init__()\n",
    "    # Build the encoder and decoder\n",
    "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                      embedding_dim, units)\n",
    "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                      embedding_dim, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_text_processor = input_text_processor\n",
    "    self.output_text_processor = output_text_processor\n",
    "    self.use_tf_function = use_tf_function\n",
    "    self.shape_checker = ShapeChecker()\n",
    "\n",
    "  def train_step(self, inputs):\n",
    "    self.shape_checker = ShapeChecker()\n",
    "    if self.use_tf_function:\n",
    "      return self._tf_train_step(inputs)\n",
    "    else:\n",
    "      return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(self, input_text, target_text):\n",
    "  self.shape_checker(input_text, ('batch',))\n",
    "  self.shape_checker(target_text, ('batch',))\n",
    "\n",
    "  # Convert the text to token IDs\n",
    "  input_tokens = self.input_text_processor(input_text)\n",
    "  target_tokens = self.output_text_processor(target_text)\n",
    "  self.shape_checker(input_tokens, ('batch', 's'))\n",
    "  self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "  # Convert IDs to masks.\n",
    "  input_mask = input_tokens != 0\n",
    "  self.shape_checker(input_mask, ('batch', 's'))\n",
    "\n",
    "  target_mask = target_tokens != 0\n",
    "  self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "  return input_tokens, input_mask, target_tokens, target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_step(self, inputs):\n",
    "  input_text, target_text = inputs  \n",
    "\n",
    "  (input_tokens, input_mask,\n",
    "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "  max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Encode the input\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "    # Initialize the decoder's state to the encoder's final state.\n",
    "    # This only works if the encoder and decoder have the same number of\n",
    "    # units.\n",
    "    dec_state = enc_state\n",
    "    loss = tf.constant(0.0)\n",
    "\n",
    "    for t in tf.range(max_target_length-1):\n",
    "      # Pass in two tokens from the target sequence:\n",
    "      # 1. The current input to the decoder.\n",
    "      # 2. The target the target for the decoder's next prediction.\n",
    "      new_tokens = target_tokens[:, t:t+2]\n",
    "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                             enc_output, dec_state)\n",
    "      loss = loss + step_loss\n",
    "\n",
    "    # Average the loss over all non padding tokens.\n",
    "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "  # Apply an optimization step\n",
    "  variables = self.trainable_variables \n",
    "  gradients = tape.gradient(average_loss, variables)\n",
    "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  # Return a dict mapping metric names to current value\n",
    "  return {'batch_loss': average_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "  # Run the decoder one step.\n",
    "  decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                               enc_output=enc_output,\n",
    "                               mask=input_mask)\n",
    "\n",
    "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "  # `self.loss` returns the total for non-padded tokens\n",
    "  y = target_token\n",
    "  y_pred = dec_result.logits\n",
    "  step_loss = self.loss(y, y_pred)\n",
    "\n",
    "  return step_loss, dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.517193191416238"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(output_text_processor.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.6270204>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.5970106>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.542352>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.390661>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.877533>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.1571403>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.404198>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.4068804>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.230295>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1394734>}\n",
      "\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "def _tf_train_step(self, inputs):\n",
    "  return self._train_step(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTranslator._tf_train_step = _tf_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.use_tf_function = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.13974>}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.train_step([example_input_batch, example_target_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1289954>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.087501>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.03315>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9564688>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.8728633>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.8190496>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.7674317>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.7205229>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.680968>}\n",
      "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.640155>}\n",
      "\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for n in range(10):\n",
    "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2cd077ecf60>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = []\n",
    "for n in range(100):\n",
    "  print('.', end='')\n",
    "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
    "  losses.append(logs['batch_loss'].numpy())\n",
    "\n",
    "print()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, key):\n",
    "    self.key = key\n",
    "    self.logs = []\n",
    "\n",
    "  def on_train_batch_end(self, n, logs):\n",
    "    self.logs.append(logs[self.key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1859/1859 [==============================] - 4266s 2s/step - batch_loss: 2.0516\n",
      "Epoch 2/3\n",
      "1859/1859 [==============================] - 4454s 2s/step - batch_loss: 1.0457\n",
      "Epoch 3/3\n",
      "1160/1859 [=================>............] - ETA: 33:49 - batch_loss: 0.8097"
     ]
    }
   ],
   "source": [
    "batch_loss = BatchLogs('batch_loss')\n",
    "train_translator.fit(dataset, epochs=3,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
